{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for LSTM<br>\n",
    "Authors: Rajat and Nima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dropout, Dense, Input, LSTM, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model,  model_from_json\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys, os, re, csv, codecs\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import  h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here you need to specify filepath for the dataset\n",
    "PATH = \"/home/ubuntu/fastai/courses/Ait590/\"\n",
    "train = pd.read_csv(PATH+\"Toxic_PreProc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t   proj-draft1.ipynb\t  Untitled1.ipynb   Word2Vec.ipynb\r\n",
      "Glove_Rajat.ipynb  Toxic_PreProc.csv\t  Untitled.ipynb\r\n",
      "model.bin\t   Toxic_PreProc.csv.zip  Unzip_File.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/ubuntu/fastai/courses/Ait590/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\",\"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "sentence_train = train[\"comment_text\"].astype(str)\n",
    "#sentence_test = test[\"comment_text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sentence_train, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing only 20000 frequent words\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing Sentence\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "token_train = tokenizer.texts_to_sequences(X_train)\n",
    "token_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[97, 121, 458, 435, 3997, 11419, 4, 3, 167, 137, 45, 9, 1, 108, 19128], [2435, 13863, 602, 946, 875, 6745, 6370, 1271], [286, 3, 64, 3582, 1054, 48, 286, 18319, 3865, 3, 3541, 1054, 27, 177, 4, 3, 20]]\n"
     ]
    }
   ],
   "source": [
    "print token_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding sentences to length 100\n",
    "max_words = 100\n",
    "X_train = pad_sequences(token_train, maxlen=max_words)\n",
    "X_test = pad_sequences(token_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Fully Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(max_words,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding Layer\n",
    "x = Embedding(max_features, 128)(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the bidirectional LSTM model. We are doing 10% regular/recurrent dropout  \n",
    "x = LSTM(60,return_sequences=True, name=\"LSTM\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the maxpooling layer\n",
    "x = GlobalMaxPool1D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One more dropout layer to help with any overfitting\n",
    "x = Dropout(0.1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing it through a nonlinear (relu) layer\n",
    "x = Dense(50, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One more dropout layer to help with any overfitting\n",
    "x = Dropout(0.1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the output layer. we need to use a nonlinear softmax or sigmoid for classification\n",
    "# Since we have a multi label classification, we'll have to use sigmoid. softmax won't work.\n",
    "x= Dense(6, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling our model\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(optimizer='adam',metrics=['accuracy'],loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 100, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 100, 60)           45360     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 50)                3050      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 2,608,716\n",
      "Trainable params: 2,608,716\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 122071 samples, validate on 13564 samples\n",
      "Epoch 1/4\n",
      "122071/122071 [==============================] - 563s 5ms/step - loss: 0.0762 - acc: 0.9764 - val_loss: 0.0531 - val_acc: 0.9801\n",
      "Epoch 2/4\n",
      "122071/122071 [==============================] - 557s 5ms/step - loss: 0.0461 - acc: 0.9829 - val_loss: 0.0505 - val_acc: 0.9811\n",
      "Epoch 3/4\n",
      "122071/122071 [==============================] - 558s 5ms/step - loss: 0.0396 - acc: 0.9847 - val_loss: 0.0511 - val_acc: 0.9806\n",
      "Epoch 4/4\n",
      "122071/122071 [==============================] - 557s 5ms/step - loss: 0.0340 - acc: 0.9866 - val_loss: 0.0547 - val_acc: 0.9806\n"
     ]
    }
   ],
   "source": [
    "#Fitting our model on dataset\n",
    "batch_size = 32\n",
    "epochs = 4\n",
    "log = model.fit(X_train,y_train, batch_size=batch_size, validation_split=0.1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"LSTM_model_Best_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23936/23936 [==============================] - 35s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = loaded_model.predict(X_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic Confusion Matrixs: \n",
      "[[21272   404]\n",
      " [  539  1721]]\n",
      "\n",
      "Severe Toxic: \n",
      "[[23646    65]\n",
      " [  162    63]]\n",
      "\n",
      "Obscene: \n",
      "[[22432   225]\n",
      " [  236  1043]]\n",
      "\n",
      "Threat: \n",
      "[[23883     0]\n",
      " [   53     0]]\n",
      "\n",
      "Insult: \n",
      "[[22453   291]\n",
      " [  376   816]]\n",
      "\n",
      "Identity Hate: \n",
      "[[23660    49]\n",
      " [  164    63]]\n"
     ]
    }
   ],
   "source": [
    "#plotting the confusion matrix\n",
    "print(\"Toxic:\\n{}\".format(confusion_matrix(y_test[:,0], y_pred[:,0])))\n",
    "print(\"\\nSevere Toxic: \\n{}\".format(confusion_matrix(y_test[:,1], y_pred[:,1])))\n",
    "print(\"\\nObscene: \\n{}\".format(confusion_matrix(y_test[:,2], y_pred[:,2])))\n",
    "print(\"\\nThreat: \\n{}\".format(confusion_matrix(y_test[:,3], y_pred[:,3])))\n",
    "print(\"\\nInsult: \\n{}\".format(confusion_matrix(y_test[:,4], y_pred[:,4])))\n",
    "print(\"\\nIdentity Hate: \\n{}\".format(confusion_matrix(y_test[:,5], y_pred[:,5])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23936"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23936/23936 [==============================] - 28s 1ms/step\n",
      "('The number of correct predictions: ', 21964)\n",
      "('The number of attempted predictions: ', 23936)\n",
      "('Accuracy: ', 0.9176136363636364)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = model.predict(X_test, verbose=1)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    for j in range(6):\n",
    "        y_pred[i][j] = round(y_pred[i][j])\n",
    "\n",
    "count = 0\n",
    "for  i in range(len(y_pred)):\n",
    "    if (y_test[i]==y_pred[i]).all():\n",
    "        count +=1\n",
    "print (\"The number of correct predictions: \",count)\n",
    "print (\"The number of attempted predictions: \",len(y_pred))\n",
    "print (\"Accuracy: \", float(count)/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LSTM + CNN Fully Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 128)           2560000   \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 50, 60)            45360     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                3050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 2,608,716\n",
      "Trainable params: 2,608,716\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dropout,Conv1D, Dense, Input, LSTM, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model,  model_from_json\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys, os, re, csv, codecs\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import  h5py\n",
    "\n",
    "PATH = \"/home/ubuntu/fastai/courses/Ait590/\"\n",
    "train_set = pd.read_csv(PATH+\"Toxic_PreProc.csv\")\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\",\"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_set[list_classes].values\n",
    "list_sent_tr = train_set[\"comment_text\"].astype(str)\n",
    "train_set[\"comment_text\"]=train_set[\"comment_text\"].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list_sent_tr)\n",
    "list_sent_tok=tokenizer.texts_to_sequences(list_sent_tr)\n",
    "\n",
    "max_words=50\n",
    "Pad_Train=pad_sequences(list_sent_tok,maxlen=max_words)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Pad_Train, y, test_size = 0.15)\n",
    "\n",
    "inp = Input(shape=(max_words,))\n",
    "\n",
    "x = Embedding(max_features, 128)(inp)\n",
    "\n",
    "#Running the bidirectional LSTM model. We are doing 10% regular/recurrent dropout  \n",
    "x = LSTM(60,return_sequences=True, name=\"LSTM\")(x)\n",
    "\n",
    "#Adding a CNN layer\n",
    "Conv_1 = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "\n",
    "\n",
    "#Adding the maxpooling layer\n",
    "x = GlobalMaxPool1D()(x)\n",
    "\n",
    "#passing it through a nonlinear (relu) layer\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "\n",
    "#One more dropout layer to help with any overfitting\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "#this is the output layer. we need to use a nonlinear softmax or sigmoid for classification\n",
    "# Since we have a multi label classification, we'll have to use sigmoid. softmax won't work.\n",
    "x= Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(optimizer='adam',metrics=['accuracy'],loss='binary_crossentropy')\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 4\n",
    "hist = model.fit(X_train,y_train, batch_size=batch_size, validation_split=0.1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.suptitle('Optimizer : Adam', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.plot(hist.history['loss'], color='b', label='Training Loss')\n",
    "plt.plot(hist.history['val_loss'], color='r', label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23936/23936 [==============================] - 14s 588us/step\n",
      "('The number of correct predictions: ', 21874)\n",
      "('The number of attempted predictions: ', 23936)\n",
      "('Accuracy: ', 0.9138536096256684)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc4d0bb7ad0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test, verbose=1)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    for j in range(6):\n",
    "        y_pred[i][j] = round(y_pred[i][j])\n",
    "\n",
    "count = 0\n",
    "for  i in range(len(y_pred)):\n",
    "    if (y_test[i]==y_pred[i]).all():\n",
    "        count +=1\n",
    "print (\"The number of correct predictions: \",count)\n",
    "print (\"The number of attempted predictions: \",len(y_pred))\n",
    "print (\"Accuracy: \", float(count)/len(y_pred))\n",
    "\n",
    "\n",
    "plt.suptitle('Optimizer : Adam', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.plot(hist.history['loss'], color='b', label='Training Loss')\n",
    "plt.plot(hist.history['val_loss'], color='r', label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LSTM + CNN Fully Trained (using early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 50, 128)           2560000   \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 50, 60)            45360     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                3050      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 2,608,716\n",
      "Trainable params: 2,608,716\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 122071 samples, validate on 13564 samples\n",
      "Epoch 1/4\n",
      "122071/122071 [==============================] - 279s 2ms/step - loss: 0.0767 - acc: 0.9760 - val_loss: 0.0513 - val_acc: 0.9811\n",
      "Epoch 2/4\n",
      "122071/122071 [==============================] - 278s 2ms/step - loss: 0.0470 - acc: 0.9825 - val_loss: 0.0494 - val_acc: 0.9819\n",
      "Epoch 3/4\n",
      "122071/122071 [==============================] - 278s 2ms/step - loss: 0.0398 - acc: 0.9845 - val_loss: 0.0511 - val_acc: 0.9814\n",
      "Epoch 4/4\n",
      "122071/122071 [==============================] - 280s 2ms/step - loss: 0.0341 - acc: 0.9867 - val_loss: 0.0519 - val_acc: 0.9810\n",
      "23936/23936 [==============================] - 14s 586us/step\n",
      "('The number of correct predictions: ', 21853)\n",
      "('The number of attempted predictions: ', 23936)\n",
      "('Accuracy: ', 0.9129762700534759)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dropout,Conv1D, Dense, Input, LSTM, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model,  model_from_json\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys, os, re, csv, codecs\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import  h5py\n",
    "\n",
    "PATH = \"/home/ubuntu/fastai/courses/Ait590/\"\n",
    "train_set = pd.read_csv(PATH+\"Toxic_PreProc.csv\")\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\",\"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_set[list_classes].values\n",
    "list_sent_tr = train_set[\"comment_text\"].astype(str)\n",
    "train_set[\"comment_text\"]=train_set[\"comment_text\"].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list_sent_tr)\n",
    "list_sent_tok=tokenizer.texts_to_sequences(list_sent_tr)\n",
    "\n",
    "max_words=50\n",
    "Pad_Train=pad_sequences(list_sent_tok,maxlen=max_words)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Pad_Train, y, test_size = 0.15)\n",
    "\n",
    "inp = Input(shape=(max_words,))\n",
    "\n",
    "x = Embedding(max_features, 128)(inp)\n",
    "\n",
    "#Running the bidirectional LSTM model. We are doing 10% regular/recurrent dropout  \n",
    "x = LSTM(60,return_sequences=True, name=\"LSTM\")(x)\n",
    "\n",
    "#Adding a CNN layer\n",
    "Conv_1 = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "\n",
    "\n",
    "#Adding the maxpooling layer\n",
    "x = GlobalMaxPool1D()(x)\n",
    "\n",
    "#passing it through a nonlinear (relu) layer\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "\n",
    "#One more dropout layer to help with any overfitting\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "#this is the output layer. we need to use a nonlinear softmax or sigmoid for classification\n",
    "# Since we have a multi label classification, we'll have to use sigmoid. softmax won't work.\n",
    "x= Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(optimizer='adam',metrics=['accuracy'],loss='binary_crossentropy')\n",
    "\n",
    "print (model.summary())\n",
    "\n",
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 4\n",
    "hist = model.fit(X_train,y_train, batch_size=batch_size, validation_split=0.1, epochs=epochs, verbose = 1, callbacks = [early_stop])\n",
    "\n",
    "\n",
    "##### Saving the model\n",
    "\n",
    "model_json = model.to_json()\n",
    "\n",
    "with open(\"LSTM_CNN_fully.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"LSTM_CNN_fully_weights.h5\")\n",
    "\n",
    "model.save(\"LSTM_CNN_fully.h5\")\n",
    "\n",
    "y_pred = model.predict(X_test, verbose=1)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    for j in range(6):\n",
    "        y_pred[i][j] = round(y_pred[i][j])\n",
    "\n",
    "count = 0\n",
    "for  i in range(len(y_pred)):\n",
    "    if (y_test[i]==y_pred[i]).all():\n",
    "        count +=1\n",
    "print (\"The number of correct predictions: \",count)\n",
    "print (\"The number of attempted predictions: \",len(y_pred))\n",
    "print (\"Accuracy: \", float(count)/len(y_pred))\n",
    "\n",
    "\n",
    "plt.suptitle('Optimizer : Adam', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.plot(hist.history['loss'], color='b', label='Training Loss')\n",
    "plt.plot(hist.history['val_loss'], color='r', label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(\"test_plot\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LSTM + CNN + GRU Fully Trained (using early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 50, 128)           2560000   \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 50, 60)            45360     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 256)           145152    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                12850     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 2,763,668\n",
      "Trainable params: 2,763,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 122071 samples, validate on 13564 samples\n",
      "Epoch 1/4\n",
      "122071/122071 [==============================] - 660s 5ms/step - loss: 0.0659 - acc: 0.9789 - val_loss: 0.0504 - val_acc: 0.9815\n",
      "Epoch 2/4\n",
      "122071/122071 [==============================] - 664s 5ms/step - loss: 0.0444 - acc: 0.9834 - val_loss: 0.0475 - val_acc: 0.9821\n",
      "Epoch 3/4\n",
      "122071/122071 [==============================] - 664s 5ms/step - loss: 0.0368 - acc: 0.9855 - val_loss: 0.0513 - val_acc: 0.9822\n",
      "Epoch 4/4\n",
      "122071/122071 [==============================] - 675s 6ms/step - loss: 0.0303 - acc: 0.9881 - val_loss: 0.0560 - val_acc: 0.9813\n",
      "23936/23936 [==============================] - 35s 1ms/step\n",
      "('The number of correct predictions: ', 21848)\n",
      "('The number of attempted predictions: ', 23936)\n",
      "('Accuracy: ', 0.9127673796791443)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dropout,Conv1D, Dense, Input, LSTM, Activation,  GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model,  model_from_json\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys, os, re, csv, codecs\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import  h5py\n",
    "\n",
    "PATH = \"/home/ubuntu/fastai/courses/Ait590/\"\n",
    "train_set = pd.read_csv(PATH+\"Toxic_PreProc.csv\")\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\",\"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_set[list_classes].values\n",
    "list_sent_tr = train_set[\"comment_text\"].astype(str)\n",
    "train_set[\"comment_text\"]=train_set[\"comment_text\"].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list_sent_tr)\n",
    "list_sent_tok=tokenizer.texts_to_sequences(list_sent_tr)\n",
    "\n",
    "max_words=50\n",
    "Pad_Train=pad_sequences(list_sent_tok,maxlen=max_words)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Pad_Train, y, test_size = 0.15, random_state=42)\n",
    "\n",
    "inp = Input(shape=(max_words,))\n",
    "\n",
    "x = Embedding(max_features, 128)(inp)\n",
    "\n",
    "#Running the bidirectional LSTM model. We are doing 10% regular/recurrent dropout  \n",
    "x = LSTM(60,return_sequences=True, name=\"LSTM\")(x)\n",
    "\n",
    "#GRU\n",
    "x = Bidirectional(GRU(128, return_sequences = True))(x)\n",
    "\n",
    "#Adding a CNN layer\n",
    "Conv_1 = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "\n",
    "\n",
    "#Adding the maxpooling layer\n",
    "x = GlobalMaxPool1D()(x)\n",
    "\n",
    "#passing it through a nonlinear (relu) layer\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "\n",
    "#One more dropout layer to help with any overfitting\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "#this is the output layer. we need to use a nonlinear softmax or sigmoid for classification\n",
    "# Since we have a multi label classification, we'll have to use sigmoid. softmax won't work.\n",
    "x= Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(optimizer='adam',metrics=['accuracy'],loss='binary_crossentropy')\n",
    "\n",
    "print (model.summary())\n",
    "\n",
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 4\n",
    "hist = model.fit(X_train,y_train, batch_size=batch_size, validation_split=0.1, epochs=epochs, verbose = 1, callbacks = [early_stop])\n",
    "\n",
    "\n",
    "##### Saving the model\n",
    "\n",
    "model_json = model.to_json()\n",
    "\n",
    "with open(\"LSTM_CNN_fully.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"LSTM_CNN_fully_weights.h5\")\n",
    "\n",
    "model.save(\"LSTM_CNN_fully.h5\")\n",
    "\n",
    "y_pred = model.predict(X_test, verbose=1)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    for j in range(6):\n",
    "        y_pred[i][j] = round(y_pred[i][j])\n",
    "\n",
    "count = 0\n",
    "for  i in range(len(y_pred)):\n",
    "    if (y_test[i]==y_pred[i]).all():\n",
    "        count +=1\n",
    "print (\"The number of correct predictions: \",count)\n",
    "print (\"The number of attempted predictions: \",len(y_pred))\n",
    "print (\"Accuracy: \", float(count)/len(y_pred))\n",
    "\n",
    "\n",
    "plt.suptitle('Optimizer : Adam', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.plot(hist.history['loss'], color='b', label='Training Loss')\n",
    "plt.plot(hist.history['val_loss'], color='r', label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(\"test_plot\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LSTM Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65522\n"
     ]
    }
   ],
   "source": [
    "word2vec_dict = word2vec.KeyedVectors.load_word2vec_format(\"/home/ubuntu/fastai/courses/Ait590/Embedding/GoogleNews-vectors-negative300.bin\"\n",
    "                                                          , binary=True)\n",
    "pretrain_emb = dict()\n",
    "for tok in word2vec_dict.wv.vocab:\n",
    "    pretrain_emb[tok] = word2vec_dict.word_vec(tok)\n",
    "all_embs=np.stack(list(pretrain_emb.values()))\n",
    "Gl_emb_mean, Gl_Std=all_embs.mean(), all_embs.std()\n",
    "num_words=len(tokenizer.word_index)\n",
    "embed_size=300\n",
    "pretrain_matrix = np.random.normal(Gl_emb_mean, Gl_Std, (num_words, embed_size))\n",
    "\n",
    "count = 0\n",
    "for tok , i in tokenizer.word_index.items():\n",
    "    \n",
    "    if pretrain_emb.get(tok) is None:\n",
    "        continue\n",
    "    \n",
    "    pretrain_matrix[i-1] = pretrain_emb[tok] \n",
    "    count +=1\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[168663,300]\n\t [[Node: embedding_2/embeddings/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@embedding_2/embeddings\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](embedding_2/embeddings, embedding_2/random_uniform)]]\n\nCaused by op u'embedding_2/embeddings/Assign', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-5991fe931683>\", line 2, in <module>\n    x = Embedding(pretrain_matrix.shape[0], pretrain_matrix.shape[1],trainable=False,weights=[pretrain_matrix])(inp)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 592, in __call__\n    self.build(input_shapes[0])\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/embeddings.py\", line 105, in build\n    dtype=self.dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 416, in add_weight\n    constraint=constraint)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 396, in variable\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 350, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py\", line 276, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 57, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2991, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1479, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[168663,300]\n\t [[Node: embedding_2/embeddings/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@embedding_2/embeddings\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](embedding_2/embeddings, embedding_2/random_uniform)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5991fe931683>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrain_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrain_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpretrain_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BI_LSTM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Adding the maxpooling layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalMaxPool1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;31m# Load weights that were specified at layer instantiation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   2325\u001b[0m     \"\"\"\n\u001b[1;32m   2326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2328\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2329\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[168663,300]\n\t [[Node: embedding_2/embeddings/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@embedding_2/embeddings\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](embedding_2/embeddings, embedding_2/random_uniform)]]\n\nCaused by op u'embedding_2/embeddings/Assign', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-5991fe931683>\", line 2, in <module>\n    x = Embedding(pretrain_matrix.shape[0], pretrain_matrix.shape[1],trainable=False,weights=[pretrain_matrix])(inp)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 592, in __call__\n    self.build(input_shapes[0])\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/embeddings.py\", line 105, in build\n    dtype=self.dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 416, in add_weight\n    constraint=constraint)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 396, in variable\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 350, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py\", line 276, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 57, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2991, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1479, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[168663,300]\n\t [[Node: embedding_2/embeddings/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@embedding_2/embeddings\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](embedding_2/embeddings, embedding_2/random_uniform)]]\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(max_words,))\n",
    "x = Embedding(pretrain_matrix.shape[0], pretrain_matrix.shape[1],trainable=False,weights=[pretrain_matrix])(inp)\n",
    "x = LSTM(60,return_sequences=True,dropout=.1,recurrent_dropout=.1,name=\"BI_LSTM\")(x)\n",
    "#Adding the maxpooling layer\n",
    "x = GlobalMaxPool1D()(x)\n",
    "#One more dropout layer to help with any overfitting\n",
    "x = Dropout(0.2)(x)\n",
    "#passing it through a nonlinear (relu) layer\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "#One more dropout layer to help with any overfitting\n",
    "x = Dropout(0.2)(x)\n",
    "#this is the output layer. we need to use a nonlinear softmax or sigmoid for classification\n",
    "# Since we have a multi label classification, we'll have to use sigmoid. softmax won't work.\n",
    "x= Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(optimizer='adam',metrics=['accuracy'],loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "log = model.fit(X_train,y_train, batch_size=batch_size, validation_split=0.1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.suptitle('Optimizer : Adam', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.plot(log.history['loss'], color='b', label='Training Loss')\n",
    "plt.plot(log.history['val_loss'], color='r', label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "#plt.save(\"Bi-LSTM-Word2vec-train-valid-loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "model.save('word2vec_BI_lstm_model.h5')\n",
    "model_json = model.to_json()\n",
    "with open(\"word2vec_BI_lstm_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"word2vec_BI_lstm_model_weights.h5\")\n",
    "model=load_model('word2vec_BI_lstm_model.h5')\n",
    "y_pred = model.predict(X_test, verbose=1)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    for j in range(6):\n",
    "        y_pred[i][j] = round(y_pred[i][j])\n",
    "\n",
    "count = 0\n",
    "for  i in range(len(y_pred)):\n",
    "    if (y_test[i]==y_pred[i]).all():\n",
    "        count +=1\n",
    "print (\"The number of correct predictions: \",count)\n",
    "print (\"The number of attempted predictions: \",len(y_pred))\n",
    "print (\"Accuracy: \", float(count)/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Toxic:\\n{}\".format(confusion_matrix(y_test[:,0], y_pred[:,0])))\n",
    "print(\"\\nSevere Tox: \\n    {}\".format(confusion_matrix(y_test[:,1], y_pred[:,1])))\n",
    "print(\"\\nObscene: \\n    {}\".format(confusion_matrix(y_test[:,2], y_pred[:,2])))\n",
    "print(\"\\nThreat: \\n     {}\".format(confusion_matrix(y_test[:,3], y_pred[:,3])))\n",
    "print(\"\\nInsult: \\n    {}\".format(confusion_matrix(y_test[:,4], y_pred[:,4])))\n",
    "print(\"\\nIden Hate: \\n    {}\".format(confusion_matrix(y_test[:,5], y_pred[:,5])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
