{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model with Fully Trained Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/3\n",
      "114890/114890 [==============================] - 779s 7ms/step - loss: 0.0699 - acc: 0.9769 - val_loss: 0.0530 - val_acc: 0.9807\n",
      "Epoch 2/3\n",
      "114890/114890 [==============================] - 777s 7ms/step - loss: 0.0470 - acc: 0.9826 - val_loss: 0.0513 - val_acc: 0.9815\n",
      "Epoch 3/3\n",
      "114890/114890 [==============================] - 777s 7ms/step - loss: 0.0392 - acc: 0.9846 - val_loss: 0.0581 - val_acc: 0.9812\n",
      "31915/31915 [==============================] - 58s 2ms/step\n",
      "('The number of correct predictions: ', 29380)\n",
      "('The number of attempted predictions: ', 31915)\n",
      "('Accuracy: ', 0.9205702647657841)\n",
      "Toxic: [[28605   308]\n",
      " [  921  2081]]\n",
      "\n",
      "Severe Toxic: \n",
      "[[31495   111]\n",
      " [  181   128]]\n",
      "\n",
      "Obscene: \n",
      "[[30071   169]\n",
      " [  421  1254]]\n",
      "\n",
      "Threat: \n",
      "[[31821     0]\n",
      " [   94     0]]\n",
      "\n",
      "Insult: \n",
      "[[30042   334]\n",
      " [  568   971]]\n",
      "\n",
      "Identity Hate: \n",
      "[[31600    13]\n",
      " [  295     7]]\n",
      "Train on 114891 samples, validate on 12766 samples\n",
      "Epoch 1/3\n",
      "114891/114891 [==============================] - 772s 7ms/step - loss: 0.0661 - acc: 0.9781 - val_loss: 0.0518 - val_acc: 0.9808\n",
      "Epoch 2/3\n",
      "114891/114891 [==============================] - 771s 7ms/step - loss: 0.0464 - acc: 0.9826 - val_loss: 0.0553 - val_acc: 0.9812\n",
      "Epoch 3/3\n",
      "114891/114891 [==============================] - 770s 7ms/step - loss: 0.0395 - acc: 0.9845 - val_loss: 0.0536 - val_acc: 0.9816\n",
      "31914/31914 [==============================] - 58s 2ms/step\n",
      "('The number of correct predictions: ', 29222)\n",
      "('The number of attempted predictions: ', 31914)\n",
      "('Accuracy: ', 0.9156483048192016)\n",
      "Toxic: [[28394   347]\n",
      " [  960  2213]]\n",
      "\n",
      "Severe Toxic: \n",
      "[[31554    16]\n",
      " [  309    35]]\n",
      "\n",
      "Obscene: \n",
      "[[29907   249]\n",
      " [  385  1373]]\n",
      "\n",
      "Threat: \n",
      "[[31817     0]\n",
      " [   97     0]]\n",
      "\n",
      "Insult: \n",
      "[[29984   302]\n",
      " [  633   995]]\n",
      "\n",
      "Identity Hate: \n",
      "[[31599    28]\n",
      " [  244    43]]\n",
      "Train on 114891 samples, validate on 12766 samples\n",
      "Epoch 1/3\n",
      "114891/114891 [==============================] - 779s 7ms/step - loss: 0.0648 - acc: 0.9787 - val_loss: 0.0514 - val_acc: 0.9815\n",
      "Epoch 2/3\n",
      "114891/114891 [==============================] - 776s 7ms/step - loss: 0.0448 - acc: 0.9830 - val_loss: 0.0494 - val_acc: 0.9822\n",
      "Epoch 3/3\n",
      "114891/114891 [==============================] - 777s 7ms/step - loss: 0.0374 - acc: 0.9854 - val_loss: 0.0544 - val_acc: 0.9815\n",
      "31914/31914 [==============================] - 59s 2ms/step\n",
      "('The number of correct predictions: ', 29333)\n",
      "('The number of attempted predictions: ', 31914)\n",
      "('Accuracy: ', 0.9191264022059284)\n",
      "Toxic: [[28361   524]\n",
      " [  751  2278]]\n",
      "\n",
      "Severe Toxic: \n",
      "[[31588     8]\n",
      " [  305    13]]\n",
      "\n",
      "Obscene: \n",
      "[[30004   222]\n",
      " [  361  1327]]\n",
      "\n",
      "Threat: \n",
      "[[31807     0]\n",
      " [  107     0]]\n",
      "\n",
      "Insult: \n",
      "[[29903   393]\n",
      " [  493  1125]]\n",
      "\n",
      "Identity Hate: \n",
      "[[31619    28]\n",
      " [  200    67]]\n",
      "Train on 114891 samples, validate on 12766 samples\n",
      "Epoch 1/3\n",
      "114891/114891 [==============================] - 786s 7ms/step - loss: 0.0666 - acc: 0.9782 - val_loss: 0.0525 - val_acc: 0.9808\n",
      "Epoch 2/3\n",
      "114891/114891 [==============================] - 784s 7ms/step - loss: 0.0455 - acc: 0.9829 - val_loss: 0.0500 - val_acc: 0.9813\n",
      "Epoch 3/3\n",
      "114891/114891 [==============================] - 786s 7ms/step - loss: 0.0382 - acc: 0.9851 - val_loss: 0.0523 - val_acc: 0.9805\n",
      "31914/31914 [==============================] - 59s 2ms/step\n",
      "('The number of correct predictions: ', 29227)\n",
      "('The number of attempted predictions: ', 31914)\n",
      "('Accuracy: ', 0.9158049758726577)\n",
      "Toxic: [[28229   665]\n",
      " [  691  2329]]\n",
      "\n",
      "Severe Toxic: \n",
      "[[31554    61]\n",
      " [  222    77]]\n",
      "\n",
      "Obscene: \n",
      "[[29952   307]\n",
      " [  301  1354]]\n",
      "\n",
      "Threat: \n",
      "[[31815     0]\n",
      " [   99     0]]\n",
      "\n",
      "Insult: \n",
      "[[30015   372]\n",
      " [  509  1018]]\n",
      "\n",
      "Identity Hate: \n",
      "[[31619    28]\n",
      " [  198    69]]\n",
      "Train on 114891 samples, validate on 12766 samples\n",
      "Epoch 1/3\n",
      "114891/114891 [==============================] - 787s 7ms/step - loss: 0.0645 - acc: 0.9788 - val_loss: 0.0512 - val_acc: 0.9814\n",
      "Epoch 2/3\n",
      "114891/114891 [==============================] - 786s 7ms/step - loss: 0.0444 - acc: 0.9830 - val_loss: 0.0496 - val_acc: 0.9816\n",
      "Epoch 3/3\n",
      "114891/114891 [==============================] - 787s 7ms/step - loss: 0.0370 - acc: 0.9852 - val_loss: 0.0549 - val_acc: 0.9817\n",
      "31914/31914 [==============================] - 59s 2ms/step\n",
      "('The number of correct predictions: ', 29322)\n",
      "('The number of attempted predictions: ', 31914)\n",
      "('Accuracy: ', 0.9187817258883249)\n",
      "Toxic: [[28402   442]\n",
      " [  878  2192]]\n",
      "\n",
      "Severe Toxic: \n",
      "[[31550    39]\n",
      " [  253    72]]\n",
      "\n",
      "Obscene: \n",
      "[[29981   260]\n",
      " [  361  1312]]\n",
      "\n",
      "Threat: \n",
      "[[31827     6]\n",
      " [   74     7]]\n",
      "\n",
      "Insult: \n",
      "[[29923   426]\n",
      " [  463  1102]]\n",
      "\n",
      "Identity Hate: \n",
      "[[31598    34]\n",
      " [  216    66]]\n",
      "91.80% (+/- 0.19%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Make sure to run this script in python 2.7\n",
    "#You should see \"Python 2\" on the top right corner of the jupyter notebook to make sure it's the correct one\n",
    "\n",
    "#This is the best model to run and test. It's been cleaned up and tested with cross validation\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dropout, Dense, Input, LSTM, Activation, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model,  model_from_json\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys, os, re, csv, codecs\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import  h5py\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "###Make sure to change the path to where you place the csv file####\n",
    "\n",
    "PATH = \"/home/ubuntu/\"\n",
    "\n",
    "#We took the raw CSV file and performed the preprocessing steps on it\n",
    "#The submitted CSV file has gone through the preprocessing steps and it's ready for our model\n",
    "train = pd.read_csv(PATH+\"Toxic_PreProc.csv\")\n",
    "\n",
    "\n",
    "#splitting the lables from the comments\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\",\"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "sentence_train = train[\"comment_text\"].astype(str)\n",
    "\n",
    "\n",
    "#We are keeping 20000 unique tokens\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(sentence_train)\n",
    "\n",
    "#Changing the tokens to sequence integers\n",
    "list_sent_tok=tokenizer.texts_to_sequences(sentence_train)\n",
    "\n",
    "#maximum number of words per sentence is 100\n",
    "max_words=100\n",
    "Pad_Train=pad_sequences(list_sent_tok,maxlen=max_words)\n",
    "\n",
    "\n",
    "#Setting up the 5 fold cross validation on our model\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=5)\n",
    "cvscores = []\n",
    "\n",
    "\n",
    "#Running the model with cross validation\n",
    "for train, test in kfold.split(Pad_Train, y):\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(Pad_Train, y, test_size=0.15, random_state=42)\n",
    "\n",
    "    inp = Input(shape=(max_words,))\n",
    "\n",
    "    x = Embedding(max_features, 128)(inp)\n",
    "\n",
    "    #Adding a GRU layer\n",
    "    x = Bidirectional(GRU(128, return_sequences = True))(x)\n",
    "\n",
    "    #Adding the maxpooling layer\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "\n",
    "    #One more dropout layer to help with any overfitting\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    #passing it through a nonlinear (relu) layer\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "\n",
    "    #One more dropout layer to help with any overfitting\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    #this is the output layer. we need to use a nonlinear softmax or sigmoid for classification\n",
    "    # Since we have a multi label classification, we'll have to use sigmoid. softmax won't work.\n",
    "    x= Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(optimizer='adam',metrics=['accuracy'],loss='binary_crossentropy')\n",
    "    \n",
    "    batch_size = 32\n",
    "    epochs = 3\n",
    "    log = model.fit(Pad_Train[train],y[train], batch_size=batch_size, validation_split=0.1, epochs=epochs)\n",
    "    \n",
    "    #run the test set and get the prediction values    \n",
    "    y_pred = model.predict(Pad_Train[test], verbose=1)\n",
    "    \n",
    "    #prediction values need to get transformed to binary\n",
    "    for i in range(len(y_pred)):\n",
    "        for j in range(6):\n",
    "            y_pred[i][j] = round(y_pred[i][j])\n",
    "\n",
    "    count = 0\n",
    "    y_test = y[test]\n",
    "    \n",
    "    #scoring the accuracy \n",
    "    for  i in range(len(y_pred)):\n",
    "        if (y_test[i]==y_pred[i]).all():\n",
    "            count +=1\n",
    "    score = (float(count)/len(y_pred))\n",
    "\n",
    "    print (\"The number of correct predictions: \",count)\n",
    "    print (\"The number of attempted predictions: \",len(y_pred))\n",
    "    print (\"Accuracy: \", score)\n",
    "    \n",
    "    cvscores.append(score * 100)\n",
    "    \n",
    "    print(\"Toxic: {}\".format(confusion_matrix(y_test[:,0], y_pred[:,0])))\n",
    "    print(\"\\nSevere Toxic: \\n{}\".format(confusion_matrix(y_test[:,1], y_pred[:,1])))\n",
    "    print(\"\\nObscene: \\n{}\".format(confusion_matrix(y_test[:,2], y_pred[:,2])))\n",
    "    print(\"\\nThreat: \\n{}\".format(confusion_matrix(y_test[:,3], y_pred[:,3])))\n",
    "    print(\"\\nInsult: \\n{}\".format(confusion_matrix(y_test[:,4], y_pred[:,4])))\n",
    "    print(\"\\nIdentity Hate: \\n{}\".format(confusion_matrix(y_test[:,5], y_pred[:,5])))\n",
    "\n",
    "#get the result of the cross validation mean\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
